# Problem Statement
For my project my goal was to create an image classifier neural network model in pytorch that is built specifically to classify common household items with at least 75% accuracy, this classification will take place across 10 items; clock, computer keyboard, lamp, telephone, television, bed, chair, couch, table, and wardrobe. I originally was just going to make one model for this, however after discovering different strategies for accomplishing this task I ended up creating 3 different models, one using a modified ResNet, another using a modified AlexNet, and finally a Simple neural network I designed myself. My goal became to train these neural networks on the same data and see which generalized the best and try to understand why.
# Models
As stated in the problem statement, the models I trained were a modified ResNet, a modified AlexNet, and a Simple neural netwrok I designed myself. I chose the ResNet model because it is a more complex classifier with 127 layers. I froze the pretrained weights of this model, so I will call this model FresNet. I also replaced the final layer of the classifier with a Linear, ReLU, Dropout, Linear, and LogSoftMax Layer, this is so I can learn the weights of these layers and hone the classifier, using the feature extractor of ResNet. I chose the AlexNet model because it is a more simple classifier than ResNet with only 21 layers. I froze the pretrained weights of this model as well, so I will call this model FalexNet. I also added another Linear layer the end of this model to get 10 classifications instead of 1000 and a LogSoftMax layer, I will learn the weights of this linear layer to hone the classifier. Finally, I defined my own neural network model that consisted of 8 layers, this model was very simple and consisted of a feature extractor and classifier, the feature extractor had the layers Conv2d, ReLU, MaxPool2d, Conv2d, ReLU, and MaxPool2d. The classifier was just a Linear layer followed by a LogSoftMax layer. I will call this model SimpNet because it is simple. I will learn the weights of all of these layers in hopes to make a simple but good classifier. The idea of using these 3 models is that the complexity varies from high, medium, to low. I want to see how this complexity affects the learning and classification process. All of these models have a final LogSoftMax layer, this is simply because I want to use the NLLLoss optimization function, as it is good for training classification neural networks.
# Data
The data I used for this project was collected from the CIFAR-100 dataset on kaggle. This dataset contains 100 different classes, but because I am making a model that works specifically for furniture, I needed to extract the 10 classes described in the problem statement from the dataset. This gave me 4000 training images, 1000 validation images, and 1000 test images. This data also required the use of an unpickling function described by the owner of the dataset on kaggle, this function was necessary to properly load the data. After the data is loaded, I needed to reshape the data so the channels of the image were referenced by the first index. This was necessary to convert the data into an Image class as defined in python's PIL library. Once the data is converted into the Image class, I finally transform the data using a transform function I defined to preprocess instances of the Image class into the format expected by my neural networks. The labels of the images were much simpler, and all I had to do was extract the labels corresponding to the Images I was using and ensure they retained the same index as their respective Image. After I had my data preprocessed, I split the training data into train and validation data and then created dataloaders for the train, validation, and test data with a batch size of 16. This concluded my data preparation. The link to this data can be found here: https://www.kaggle.com/datasets/fedesoriano/cifar100
# Hyperparameter Tuning
The only hyperparameter I tuned on these models was the learning rate. To do this I iterated over the learning rates 0.01, 0.001, 0.0001, and 0.00001 for each model. I trained them over 1 epoch and then chose the learning rate that had the highest validation accuracy for each model. This was 0.001 for both FresNet and FalexNet, however the optimal learning rate for SimpNet was 0.0001.
# Training
I trained each model over 10 epochs with a batch size of 16 from the training dataloader and the respective optimal learning rates for each model. After some research online I learned of a technique call learning rate scheduling where the learning rate is decreased during the training process, I utilized this technique in the training of all 3 models. After the 5th epoch, the learning rate decreases by a factor of 10. The idea here is that the model will make smaller adjustments to the weights after it has already honed the weights quite a bit, this way the weights can be more finely tuned than they could with a larger learning rate.
